scrapy startproject #name （scrapy startproject douban）//项目名字
利用cd命令进入spiders文件夹里 利用scrapy genspider #爬虫文件名字 #爬取的网站域名
（例如 scrapy genspider douban movie.douban.com) //不要与项目名字重复
在Items里面定义数据结构
爬虫文件名字.py 书写爬虫逻辑文件
pipelines.py 对提交的item进行保存处理 
打开ITEM_PIPELINES = { //不开启是不会保存到数据库里面
    'douban.pipelines.DoubanPipeline': 300,
}
 执行爬虫  ：scrapy crawl #爬虫文件名字
修改ROBOTSTXT为False
修改User_Agent
中间件middlewares.py
 爬虫的伪装 ip和user-agent
scrapy.py项目配置文件路径和部署信息
Items.py 定义item数据结构的地方 
setting文件 是项目的设置文件
	USer_AGENT=‘’（百度或者浏览器上找）
	并发量默认16个 CONCURRENT_REQUESTS=#要设置的并发量
		CONCURRENT_REQUESTS_PER_DOMAIN(每一个域名的并发量)
		CONCURRENT_REQUESTS_PER_IP（(每一个IP的并发量)）
	下载延迟（发送并发量个请求后 几秒后再发送并发量个请求）DOWNLOAD_DELAY=#延迟秒数
	是否开启COOKIES COOKIES_ENABLED=False
默认的请求头 DEFAULT_REQUEST_HEADERS
中间件 SPIDER_MIDDLEWARES 爬虫中间件
DOWNLOADER_MIDDLEWARES 下载中间件
EXTENSIONS  扩展中间件
中间件编写的地方 ITEM_PIPELINES
#爬虫文件名字.py
# 爬虫名字
    name = 'douban_spider'
    # 允许的域名
    allowed_domains = ['movie.douban.com']
    # 入口URL，扔到调度器里面
    start_urls = ['https://movie.douban.com/top250']
parse（）里面写爬虫
//movie_list = response.xpath("//div[@class='article']//ol[@class='grid_view']/li")
//douban_item['describe']=i_item.xpath(".//p[@class='quote']/span/text()").extract_first()
  //将数据保存在item里面 
//利用yield将item保存 将我们获取的数据yield，到我们的pipelines里面去
yield Item
//利用语句将新url保存到调度器里面
yield scrapy.Request("https://movie.douban.com/top250" + next_link, callback=self.parse)
douban scrawl #爬虫名字 -o test.js
pipelines.py 
def __init__(self): //定义链接数据库的变量
def process_item(self, item, spider): item参数就是yield过来的item 进行数据的保存处理
middlewares.py
创建一个类 //class my_proxy(object): def process_request(self,request,spider):
request.meta['proxy']
///下面为scrapy使用docker的配置
启动docker ：docker run -p 8050:8050 scrapinghub/splash 
在prider：class SplashSpider(Spider):中添加def start_requests(self):方法
  

    # request需要封装成SplashRequest
    def start_requests(self):
        for url in self.start_urls:
            yield SplashRequest(url
                                , self.parse
                                , args={'wait': '0.5'}
                                # ,endpoint='render.json'
                                )
在def parse(self, response):中 添加site = Selector(response)

splash配置.jpg


转换字符串
cookie = ''

itemDict = {}
items = cookie.split(';')
for item in items:
    key = item.split('=')[0].replace(' ', '')
    value = item.split('=')[1]
    itemDict[key] = value
print(itemDict)
   

带cookie 登录
yield scrapy.Request(url, cookies=self.cookies, callback=self.parse)
 


